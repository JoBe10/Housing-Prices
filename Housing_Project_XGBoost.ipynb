{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggel House Prices Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project I'll build a preprocessing pipeline and apply the preprocessed data to an extreme gradient boost regressor in order to forecast house sales prices based on selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the relevant libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "X_full = pd.read_csv('/Users/Jonas/Desktop/DataScience/Kaggle/Housing/CSVs/train.csv', index_col='Id')\n",
    "X_test_full = pd.read_csv('/Users/Jonas/Desktop/DataScience/Kaggle/Housing/CSVs/test.csv', index_col='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing target, separate target from predictors\n",
    "X_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = X_full.SalePrice\n",
    "X_full.drop(['SalePrice'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Furthermore we want to drop any categorical columns with missing value\n",
    "# Get all categorical columns\n",
    "cat_cols = [col for col in X_full if X_full[col].dtype == 'object']\n",
    "\n",
    "# Get all categorical columns with missing values (IMPORTANT: for both train and test data!)\n",
    "cat_with_missing = [col for col in cat_cols if (X_full[col].isnull().any() or X_test_full[col].isnull().any())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the categoricals with missing values\n",
    "X_full.drop(cat_with_missing, axis=1, inplace=True)\n",
    "X_test_full.drop(cat_with_missing, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break off validation set from training data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_full, y, train_size=0.8, test_size=0.2,\n",
    "                                                      random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassign the categorical columns remaining (to be used for OH Encoding)\n",
    "cat_cols = [col for col in X_train if X_train[col].dtype == 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Street', 2),\n",
       " ('CentralAir', 2),\n",
       " ('LandSlope', 3),\n",
       " ('PavedDrive', 3),\n",
       " ('LotShape', 4),\n",
       " ('LandContour', 4),\n",
       " ('ExterQual', 4),\n",
       " ('LotConfig', 5),\n",
       " ('BldgType', 5),\n",
       " ('ExterCond', 5),\n",
       " ('HeatingQC', 5),\n",
       " ('Condition2', 6),\n",
       " ('RoofStyle', 6),\n",
       " ('Foundation', 6),\n",
       " ('Heating', 6),\n",
       " ('SaleCondition', 6),\n",
       " ('RoofMatl', 7),\n",
       " ('HouseStyle', 8),\n",
       " ('Condition1', 9),\n",
       " ('Neighborhood', 25)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Investigate cardinality by getting number of unique entries in each column with categorical data\n",
    "unique_vals = list(map(lambda col: X_train[col].nunique(), cat_cols))\n",
    "d = dict(zip(cat_cols, unique_vals))\n",
    "sorted_d = sorted(d.items(), key=lambda x: x[1])\n",
    "sorted_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lists of low and high cardinality categorical columns\n",
    "low_cardinality = [col for col in cat_cols if X_train[col].nunique() < 10]\n",
    "high_cardinality = list(set(cat_cols) - set(low_cardinality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply OH Encoder to each column with categorical data that doesn't have missing values\n",
    "oh_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "oh_train = pd.DataFrame(oh_encoder.fit_transform(X_train[low_cardinality]))\n",
    "oh_valid = pd.DataFrame(oh_encoder.transform(X_valid[low_cardinality]))\n",
    "oh_test = pd.DataFrame(oh_encoder.transform(X_test_full[low_cardinality]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the indices back\n",
    "oh_train.index = X_train.index\n",
    "oh_valid.index = X_valid.index\n",
    "oh_test.index = X_test_full.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the categorical values (will later add the OH columns instead)\n",
    "X_train_num = X_train.drop(cat_cols, axis=1)\n",
    "X_valid_num = X_valid.drop(cat_cols, axis=1)\n",
    "X_test_num = X_test_full.drop(cat_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the numerical columns\n",
    "imp = SimpleImputer()\n",
    "imputed_X_train = pd.DataFrame(imp.fit_transform(X_train_num))\n",
    "imputed_X_valid = pd.DataFrame(imp.transform(X_valid_num))\n",
    "imputed_X_test = pd.DataFrame(imp.transform(X_test_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation removed column names; put them back\n",
    "imputed_X_train.columns = X_train_num.columns\n",
    "imputed_X_valid.columns = X_valid_num.columns\n",
    "imputed_X_test.columns = X_test_num.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the indices back (otherwise the concat later will produce NaNs where the indices don't match)\n",
    "imputed_X_train.index = X_train_num.index\n",
    "imputed_X_valid.index = X_valid_num.index\n",
    "imputed_X_test.index = X_test_num.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the OH Encoded columns and the imputed columns back together in one DataFrame\n",
    "X_train_final = pd.concat([imputed_X_train, oh_train], axis=1)\n",
    "X_valid_final = pd.concat([imputed_X_valid, oh_valid], axis=1)\n",
    "X_test_final = pd.concat([imputed_X_test, oh_test], axis=1)\n",
    "\n",
    "#X_train_final = X_train_final.select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "xgb = XGBRegressor(n_estimators=1000, learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.05, max_delta_step=0, max_depth=6,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=1000, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "xgb.fit(X_train_final, y_train, \n",
    "             early_stopping_rounds=5, \n",
    "             eval_set=[(X_valid_final, y_valid)], \n",
    "             verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17198.30079462757"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict validation set and score the predictions\n",
    "preds = xgb.predict(X_valid_final)\n",
    "\n",
    "score = mean_absolute_error(y_valid, preds)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "preds_test = xgb.predict(X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'Id': X_test.index,\n",
    "                       'SalePrice': preds_test})\n",
    "output.to_csv('/Users/Jonas/Desktop/DataScience/Kaggle/Housing/XGB.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
